{
  "nodes": [
    {
      "id": "chatOpenAI_0",
      "position": {
        "x": 425.1307291666667,
        "y": 105.83104166666664
      },
      "type": "customNode",
      "data": {
        "id": "chatOpenAI_0",
        "label": "ChatOpenAI",
        "version": 8.2,
        "name": "chatOpenAI",
        "type": "ChatOpenAI",
        "baseClasses": [
          "ChatOpenAI",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "openAIApi"
            ],
            "id": "chatOpenAI_0-input-credential-credential",
            "display": true
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "default": "gpt-4o-mini",
            "id": "chatOpenAI_0-input-modelName-asyncOptions",
            "display": true
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatOpenAI_0-input-temperature-number",
            "display": true
          },
          {
            "label": "Streaming",
            "name": "streaming",
            "type": "boolean",
            "default": true,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-streaming-boolean",
            "display": true
          },
          {
            "label": "Max Tokens",
            "name": "maxTokens",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-maxTokens-number",
            "display": true
          },
          {
            "label": "Top Probability",
            "name": "topP",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-topP-number",
            "display": true
          },
          {
            "label": "Frequency Penalty",
            "name": "frequencyPenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-frequencyPenalty-number",
            "display": true
          },
          {
            "label": "Presence Penalty",
            "name": "presencePenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-presencePenalty-number",
            "display": true
          },
          {
            "label": "Timeout",
            "name": "timeout",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-timeout-number",
            "display": true
          },
          {
            "label": "Strict Tool Calling",
            "name": "strictToolCalling",
            "type": "boolean",
            "description": "Whether the model supports the `strict` argument when passing in tools. If not specified, the `strict` argument will not be passed to OpenAI.",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-strictToolCalling-boolean",
            "display": true
          },
          {
            "label": "Stop Sequence",
            "name": "stopSequence",
            "type": "string",
            "rows": 4,
            "optional": true,
            "description": "List of stop words to use when generating. Use comma to separate multiple stop words.",
            "additionalParams": true,
            "id": "chatOpenAI_0-input-stopSequence-string",
            "display": true
          },
          {
            "label": "BasePath",
            "name": "basepath",
            "type": "string",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-basepath-string",
            "display": true
          },
          {
            "label": "Proxy Url",
            "name": "proxyUrl",
            "type": "string",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-proxyUrl-string",
            "display": true
          },
          {
            "label": "BaseOptions",
            "name": "baseOptions",
            "type": "json",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-baseOptions-json",
            "display": true
          },
          {
            "label": "Allow Image Uploads",
            "name": "allowImageUploads",
            "type": "boolean",
            "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details.",
            "default": false,
            "optional": true,
            "id": "chatOpenAI_0-input-allowImageUploads-boolean",
            "display": true
          },
          {
            "label": "Image Resolution",
            "description": "This parameter controls the resolution in which the model views the image.",
            "name": "imageResolution",
            "type": "options",
            "options": [
              {
                "label": "Low",
                "name": "low"
              },
              {
                "label": "High",
                "name": "high"
              },
              {
                "label": "Auto",
                "name": "auto"
              }
            ],
            "default": "low",
            "optional": false,
            "show": {
              "allowImageUploads": true
            },
            "id": "chatOpenAI_0-input-imageResolution-options",
            "display": false
          },
          {
            "label": "Reasoning Effort",
            "description": "Constrains effort on reasoning for reasoning models. Only applicable for o1 and o3 models.",
            "name": "reasoningEffort",
            "type": "options",
            "options": [
              {
                "label": "Low",
                "name": "low"
              },
              {
                "label": "Medium",
                "name": "medium"
              },
              {
                "label": "High",
                "name": "high"
              }
            ],
            "default": "medium",
            "optional": false,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-reasoningEffort-options",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatOpenAI_0-input-cache-BaseCache",
            "display": true
          }
        ],
        "inputs": {
          "cache": "",
          "modelName": "gpt-4o-mini",
          "temperature": 0.9,
          "streaming": true,
          "maxTokens": "",
          "topP": "",
          "frequencyPenalty": "",
          "presencePenalty": "",
          "timeout": "",
          "strictToolCalling": "",
          "stopSequence": "",
          "basepath": "",
          "proxyUrl": "",
          "baseOptions": "",
          "allowImageUploads": "",
          "imageResolution": "low",
          "reasoningEffort": "medium"
        },
        "outputAnchors": [
          {
            "id": "chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatOpenAI",
            "label": "ChatOpenAI",
            "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
            "type": "ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 677,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 425.1307291666667,
        "y": 105.83104166666664
      }
    },
    {
      "id": "chatPromptTemplate_0",
      "position": {
        "x": 434.8547932634772,
        "y": 825.1241657428285
      },
      "type": "customNode",
      "data": {
        "id": "chatPromptTemplate_0",
        "label": "Chat Prompt Template",
        "version": 2,
        "name": "chatPromptTemplate",
        "type": "ChatPromptTemplate",
        "baseClasses": [
          "ChatPromptTemplate",
          "BaseChatPromptTemplate",
          "BasePromptTemplate",
          "Runnable"
        ],
        "category": "Prompts",
        "description": "Schema to represent a chat prompt",
        "inputParams": [
          {
            "label": "System Message",
            "name": "systemMessagePrompt",
            "type": "string",
            "rows": 4,
            "placeholder": "You are a helpful assistant that translates {input_language} to {output_language}.",
            "id": "chatPromptTemplate_0-input-systemMessagePrompt-string",
            "display": true
          },
          {
            "label": "Human Message",
            "name": "humanMessagePrompt",
            "description": "This prompt will be added at the end of the messages as human message",
            "type": "string",
            "rows": 4,
            "placeholder": "{text}",
            "id": "chatPromptTemplate_0-input-humanMessagePrompt-string",
            "display": true
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "chatPromptTemplate_0-input-promptValues-json",
            "display": true
          },
          {
            "label": "Messages History",
            "name": "messageHistory",
            "description": "Add messages after System Message. This is useful when you want to provide few shot examples",
            "type": "tabs",
            "tabIdentifier": "selectedMessagesTab",
            "additionalParams": true,
            "default": "messageHistoryCode",
            "tabs": [
              {
                "label": "Add Messages (Code)",
                "name": "messageHistoryCode",
                "type": "code",
                "hideCodeExecute": true,
                "codeExample": "const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\n\nreturn [\n    new HumanMessage(\"What is 333382 🦜 1932?\"),\n    new AIMessage({\n        content: \"\",\n        tool_calls: [\n        {\n            id: \"12345\",\n            name: \"calulator\",\n            args: {\n                number1: 333382,\n                number2: 1932,\n                operation: \"divide\",\n            },\n        },\n        ],\n    }),\n    new ToolMessage({\n        tool_call_id: \"12345\",\n        content: \"The answer is 172.558.\",\n    }),\n    new AIMessage(\"The answer is 172.558.\"),\n]",
                "optional": true,
                "additionalParams": true
              }
            ],
            "id": "chatPromptTemplate_0-input-messageHistory-tabs",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "systemMessagePrompt": "You are a robot moving in a room and receiving the lidar data in the following format:\n\n{{{question}}}\n\nFrom the received data you are using range_min, range_max and ranges to interpret the border walls. \nExtraxt range_min, range_max and range and calculate the path of the robot.\nNavigation rules:\nFirst: The maximum speed is 1m/s.\nFourth:  The goal of the motion is the wall, which has the largest distance to the ego position.\nFifth: Rotate the yaw angle in cmd_pos in this direction\nSixth: Calculate the motion in small step of x and y about 0.5m\nSeventh: Always keep a safe distance of at least 0.5 meters from any wall or obstacle detected by the LIDAR.\nEighth: If an obstacle is closer than 0.5 meters, slow down or stop and steer away from it.\nPlan your path so that you move smoothly through the rooms, avoiding collisions with the wall.\n\nGenerate navigation commands as one Json string:\nname is cmd_pos, containing x, y  and  yaw as float\nname is cmd_val, containing vx, vy, vz as float, whereby vz is always zero.\nSend back the commands as json. Don't send back the question.",
          "humanMessagePrompt": "",
          "promptValues": "{\"question\":\"{{question}}\"}",
          "messageHistory": "messageHistoryCode"
        },
        "outputAnchors": [
          {
            "id": "chatPromptTemplate_0-output-chatPromptTemplate-ChatPromptTemplate|BaseChatPromptTemplate|BasePromptTemplate|Runnable",
            "name": "chatPromptTemplate",
            "label": "ChatPromptTemplate",
            "description": "Schema to represent a chat prompt",
            "type": "ChatPromptTemplate | BaseChatPromptTemplate | BasePromptTemplate | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 749,
      "selected": false,
      "positionAbsolute": {
        "x": 434.8547932634772,
        "y": 825.1241657428285
      },
      "dragging": false
    },
    {
      "id": "conversationChain_0",
      "position": {
        "x": 1473.307122390653,
        "y": 909.8351630630399
      },
      "type": "customNode",
      "data": {
        "id": "conversationChain_0",
        "label": "Conversation Chain",
        "version": 3,
        "name": "conversationChain",
        "type": "ConversationChain",
        "baseClasses": [
          "ConversationChain",
          "LLMChain",
          "BaseChain",
          "Runnable"
        ],
        "category": "Chains",
        "description": "Chat models specific conversational chain with memory",
        "inputParams": [
          {
            "label": "System Message",
            "name": "systemMessagePrompt",
            "type": "string",
            "rows": 4,
            "description": "If Chat Prompt Template is provided, this will be ignored",
            "additionalParams": true,
            "optional": true,
            "default": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.",
            "placeholder": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.",
            "id": "conversationChain_0-input-systemMessagePrompt-string",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "id": "conversationChain_0-input-model-BaseChatModel",
            "display": true
          },
          {
            "label": "Memory",
            "name": "memory",
            "type": "BaseMemory",
            "id": "conversationChain_0-input-memory-BaseMemory",
            "display": true
          },
          {
            "label": "Chat Prompt Template",
            "name": "chatPromptTemplate",
            "type": "ChatPromptTemplate",
            "description": "Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable",
            "optional": true,
            "id": "conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate",
            "display": true
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "conversationChain_0-input-inputModeration-Moderation",
            "display": true
          }
        ],
        "inputs": {
          "model": "{{chatOpenAI_0.data.instance}}",
          "memory": "{{bufferWindowMemory_0.data.instance}}",
          "chatPromptTemplate": "{{chatPromptTemplate_0.data.instance}}",
          "inputModeration": "",
          "systemMessagePrompt": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
        },
        "outputAnchors": [
          {
            "id": "conversationChain_0-output-conversationChain-ConversationChain|LLMChain|BaseChain|Runnable",
            "name": "conversationChain",
            "label": "ConversationChain",
            "description": "Chat models specific conversational chain with memory",
            "type": "ConversationChain | LLMChain | BaseChain | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 442,
      "selected": false,
      "positionAbsolute": {
        "x": 1473.307122390653,
        "y": 909.8351630630399
      },
      "dragging": false
    },
    {
      "id": "bufferWindowMemory_0",
      "position": {
        "x": 779.4853386237713,
        "y": 990.668768938793
      },
      "type": "customNode",
      "data": {
        "id": "bufferWindowMemory_0",
        "label": "Buffer Window Memory",
        "version": 2,
        "name": "bufferWindowMemory",
        "type": "BufferWindowMemory",
        "baseClasses": [
          "BufferWindowMemory",
          "BaseChatMemory",
          "BaseMemory"
        ],
        "category": "Memory",
        "description": "Uses a window of size k to surface the last k back-and-forth to use as memory",
        "inputParams": [
          {
            "label": "Size",
            "name": "k",
            "type": "number",
            "default": "4",
            "description": "Window of size k to surface the last k back-and-forth to use as memory.",
            "id": "bufferWindowMemory_0-input-k-number",
            "display": true
          },
          {
            "label": "Session Id",
            "name": "sessionId",
            "type": "string",
            "description": "If not specified, a random id will be used. Learn <a target=\"_blank\" href=\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\">more</a>",
            "default": "",
            "optional": true,
            "additionalParams": true,
            "id": "bufferWindowMemory_0-input-sessionId-string",
            "display": true
          },
          {
            "label": "Memory Key",
            "name": "memoryKey",
            "type": "string",
            "default": "chat_history",
            "additionalParams": true,
            "id": "bufferWindowMemory_0-input-memoryKey-string",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "k": "4",
          "sessionId": "",
          "memoryKey": "chat_history"
        },
        "outputAnchors": [
          {
            "id": "bufferWindowMemory_0-output-bufferWindowMemory-BufferWindowMemory|BaseChatMemory|BaseMemory",
            "name": "bufferWindowMemory",
            "label": "BufferWindowMemory",
            "description": "Uses a window of size k to surface the last k back-and-forth to use as memory",
            "type": "BufferWindowMemory | BaseChatMemory | BaseMemory"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 338,
      "positionAbsolute": {
        "x": 779.4853386237713,
        "y": 990.668768938793
      },
      "selected": false
    }
  ],
  "edges": [
    {
      "source": "chatPromptTemplate_0",
      "sourceHandle": "chatPromptTemplate_0-output-chatPromptTemplate-ChatPromptTemplate|BaseChatPromptTemplate|BasePromptTemplate|Runnable",
      "target": "conversationChain_0",
      "targetHandle": "conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate",
      "type": "buttonedge",
      "id": "chatPromptTemplate_0-chatPromptTemplate_0-output-chatPromptTemplate-ChatPromptTemplate|BaseChatPromptTemplate|BasePromptTemplate|Runnable-conversationChain_0-conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate"
    },
    {
      "source": "chatOpenAI_0",
      "sourceHandle": "chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "conversationChain_0",
      "targetHandle": "conversationChain_0-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-conversationChain_0-conversationChain_0-input-model-BaseChatModel"
    },
    {
      "source": "bufferWindowMemory_0",
      "sourceHandle": "bufferWindowMemory_0-output-bufferWindowMemory-BufferWindowMemory|BaseChatMemory|BaseMemory",
      "target": "conversationChain_0",
      "targetHandle": "conversationChain_0-input-memory-BaseMemory",
      "type": "buttonedge",
      "id": "bufferWindowMemory_0-bufferWindowMemory_0-output-bufferWindowMemory-BufferWindowMemory|BaseChatMemory|BaseMemory-conversationChain_0-conversationChain_0-input-memory-BaseMemory"
    }
  ]
}